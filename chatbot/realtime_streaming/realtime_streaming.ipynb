{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5416e6b9-a61d-45a6-85db-ecae9323e4f5",
   "metadata": {},
   "source": [
    "# RealTime | Streaming OpenAI API Integration\n",
    "\n",
    "## Architecture diagram\n",
    "\n",
    "\n",
    "You'd need to integrate Whisper for converting speech to text and then use GPT for generating responses. <b>OpenAI doesn't offer a single URL that combines these functions into one service</b>, so you'd need to handle the integration. For text-to-speech, you could use Google TTS or another TTS service to convert the generated text back into speech.\n",
    "\n",
    "Here's a text-based diagram of the architecture for implementing OpenAI Speech-to-Speech (S2S) interactions in your application:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac4f848-f4d4-4001-99f3-4b683b7def5a",
   "metadata": {},
   "source": [
    "<div style=\"font-family: monospace;\">\n",
    "    <pre>\n",
    "                                    +----------------------+\n",
    "                                    |    User Device       |\n",
    "                                    |  (Microphone +       |\n",
    "                                    |   Speakers)          |\n",
    "                                    +----------+-----------+\n",
    "                                               |\n",
    "                                               v\n",
    "                                    +----------------------+\n",
    "                                    | Frontend Application  |\n",
    "                                    | (React/JavaScript)    |\n",
    "                                    |  - Capture Audio      |\n",
    "                                    |  - Send Audio Chunks  |\n",
    "                                    +----------+-----------+\n",
    "                                               |\n",
    "                                               v\n",
    "                                    +----------------------+\n",
    "                                    |     Backend API      |\n",
    "                                    |       (FastAPI)      |\n",
    "                                    +----------+-----------+\n",
    "                                               |\n",
    "              +----------------+-------------+----------------+\n",
    "              |                |                              |\n",
    "              v                v                              v\n",
    "+---------------------+  +--------------------+     +-----------------------+\n",
    "| OpenAI Whisper API  |  | OpenAI GPT API     |     | Text-to-Speech API    |\n",
    "|  (Speech-to-Text)  |  |  (Text Generation) |     |  (e.g., Google TTS)   |\n",
    "+---------------------+  +--------------------+     +-----------------------+\n",
    "              |                |                              |\n",
    "              +----------------+                              |\n",
    "                           |                                   |\n",
    "                           +-----------------------------------+\n",
    "                                               |\n",
    "                                               v\n",
    "                                    +----------------------+\n",
    "                                    |   Send Audio Response  |\n",
    "                                    |   (WebSocket/HTTP)     |\n",
    "                                    +----------+-----------+\n",
    "                                               |\n",
    "                                               v\n",
    "                                    +----------------------+\n",
    "                                    |   Frontend Receives   |\n",
    "                                    |     Audio Response     |\n",
    "                                    |  (Play Audio to User) |\n",
    "                                    +----------------------+\n",
    "    </pre>\n",
    "</div>\n",
    "\n",
    "<h3>Text Diagram Explanation:</h3>\n",
    "<ul>\n",
    "    <li><strong>User Device:</strong> User interacts with the application via a microphone (for speech input) and speakers (for output).</li>\n",
    "    <li><strong>Frontend Application:</strong> Built with React or JavaScript, captures user audio, converts it into chunks, and sends it to the backend API via WebSocket or HTTP.</li>\n",
    "    <li><strong>Backend Application (FastAPI):</strong> Receives audio chunks, processes them, and sends them to the <a href=\"https://platform.openai.com/docs/api-reference/audio/create\" target=\"_blank\">OpenAI Whisper API</a> (for Speech-to-Text).\n",
    "        <ul>\n",
    "            <li><strong>OpenAI Whisper API:</strong> Converts user speech into text.</li>\n",
    "            <li><strong>OpenAI GPT API:</strong> Processes the transcribed text and generates a response in real-time using streaming mode. Refer to the <a href=\"https://platform.openai.com/docs/api-reference/chat/create\" target=\"_blank\">OpenAI GPT Streaming API</a>.</li>\n",
    "            <li><strong>Text-to-Speech API:</strong> Converts generated text responses back into audio for playback (e.g., Google Text-to-Speech).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Send Back Audio Response:</strong> Once the TTS process is complete, stream the audio back to the frontend using WebSocket or HTTP response streaming.</li>\n",
    "    <li><strong>Frontend Receives Audio Response:</strong> The frontend plays the received audio back to the user using the Web Audio API in JavaScript.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Detailed Flow:</h3>\n",
    "<ol>\n",
    "    <li><strong>Frontend Application:</strong>\n",
    "        <ul>\n",
    "            <li>Capture Speech Input:\n",
    "                <ul>\n",
    "                    <li>Use the Web Speech API in JavaScript to capture audio.</li>\n",
    "                    <li>Convert the captured speech into chunks (streaming).</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Send Speech to Backend (WebSocket or HTTP):\n",
    "                <ul>\n",
    "                    <li>Stream captured audio chunks to the backend using WebSocket or HTTP with chunked transfer encoding for real-time data.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Backend (FastAPI):</strong>\n",
    "        <ul>\n",
    "            <li>Receive Audio and Process Speech (Speech-to-Text):\n",
    "                <ul>\n",
    "                    <li>The backend receives the audio stream and passes it to the OpenAI Whisper API or another Speech-to-Text engine to convert the audio to text.</li>\n",
    "                    <li>Example:\n",
    "<pre><code>\n",
    "response = openai.Audio.transcribe(\n",
    "    model=\"whisper-1\",\n",
    "    file=audio_chunk,\n",
    "    language=\"en\"\n",
    ")\n",
    "transcribed_text = response['text']\n",
    "</code></pre></li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Generate Text Response (GPT-4):\n",
    "                <ul>\n",
    "                    <li>The transcribed text is sent to the GPT API using streaming mode for a faster response.</li>\n",
    "                    <li>Example:\n",
    "<pre><code>\n",
    "response = openai.Completion.create(\n",
    "    model=\"gpt-4\",\n",
    "    prompt=transcribed_text,\n",
    "    stream=True  # Enable streaming\n",
    ")\n",
    "</code></pre></li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Optional: Convert Text Response to Speech (Text-to-Speech):\n",
    "                <ul>\n",
    "                    <li>After receiving the text response, convert it to audio using a Text-to-Speech (TTS) API (e.g., Google Text-to-Speech).</li>\n",
    "                    <li>Example:\n",
    "<pre><code>\n",
    "from google.cloud import texttospeech\n",
    "\n",
    "tts_client = texttospeech.TextToSpeechClient()\n",
    "synthesis_input = texttospeech.SynthesisInput(text=response_text)\n",
    "voice = texttospeech.VoiceSelectionParams(language_code=\"en-US\", name=\"en-US-Wavenet-D\")\n",
    "audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)\n",
    "\n",
    "response = tts_client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)\n",
    "</code></pre></li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Send Back Audio Response (WebSocket or HTTP):\n",
    "                <ul>\n",
    "                    <li>Once the TTS process is complete, stream the audio back to the frontend using WebSocket or HTTP response streaming.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Frontend (Receive and Playback Audio):</strong>\n",
    "        <ul>\n",
    "            <li>Receive and Play Audio:\n",
    "                <ul>\n",
    "                    <li>The frontend receives the audio response via WebSocket or HTTP response streaming.</li>\n",
    "                    <li>It plays the audio response using the Web Audio API in JavaScript.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "<h3>Additional Components:</h3>\n",
    "<ul>\n",
    "    <li><strong>Authentication:</strong> Integrate OAuth2 or API key-based authentication to secure the API endpoints.</li>\n",
    "    <li><strong>Error Handling:</strong> Handle errors such as missing audio, invalid responses from APIs, or network interruptions gracefully.</li>\n",
    "    <li><strong>Caching (Optional):</strong> Use Redis or in-memory caching to store frequently used text-to-speech or speech-to-text conversions.</li>\n",
    "</ul>\n",
    "\n",
    "<p>This architecture combines text and speech interactions in real-time, enabling rich, dynamic conversation flows in a voice-enabled chatbot or assistant application.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e01ed6-248b-4353-90ad-b135006b1079",
   "metadata": {},
   "source": [
    "### Userful links\n",
    "\n",
    "https://openai.com/index/introducing-the-realtime-api/<br>\n",
    "https://github.com/openai/openai-realtime-api-beta<br>\n",
    "https://platform.openai.com/docs/guides/text-to-speech/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb513cd-539a-4b3c-b6bd-f67931bbde55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
